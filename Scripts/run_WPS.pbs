#!/bin/bash
#MOAB/Torque submission script for SciNet GPC

## queue/PBS settings
#PBS -l nodes=1:m128g:ppn=16
#PBS -l walltime=0:30:00
# std and error output
# #PBS -j oe
#PBS -o $PBS_JOBNAME.$PBS_JOBID.out
#PBS -e $PBS_JOBNAME.$PBS_JOBID.err
# send email if abort (nbae)
#PBS -M aerler@atmosp.physics.utoronto.ca
#PBS -m ae
# assemble job name
#PBS -N largemem_128GB_16x1
## submit to queue (NB: this has to be the last PBS line!)
# batch (default), debug, largemem
#PBS -q largemem


## job settings

# run configuration
export TASKS=16 # number of MPI task
export THREADS=1 # number of OpenMP threads
# directory setup
export INIDIR=$PBS_O_WORKDIR
export JOBNAME=$PBS_JOBNAME
export WORKDIR=$INIDIR/$JOBNAME/

# optional settings
export RUNPYWPS=0 # don't run pyWPS - load met-files from alternate source
export REALDIR=RAM # write real.exe output to RAM and copy to HD in bulk


## setup job environment
hostname
uname

# load modules
module purge
module load intel/12.1.3 intelmpi/4.0.3.008 hdf5/187-v18-serial-intel netcdf/4.1.3_hdf5_serial-intel
module load gcc/4.6.1 centos5-compat/lib64 ncl/6.0.0 python/2.7.2 # pyWPS.py specific modules
module list
# unlimit stack size (unfortunately necessary with WRF to prevent segmentation faults)
ulimit -s unlimited

# set up hybrid envionment: OpenMP and MPI (Intel)
#export OMP_NUM_THREADS=$THREADS
#export KMP_AFFINITY=verbose,granularity=thread,compact
#export I_MPI_PIN_DOMAIN=omp
export I_MPI_DEBUG=1 # less output (currently no problems) 
# launch Intel hybrid (mpi/openmp) job
export HYBRIDRUN="mpirun -ppn $TASKS -np $TASKS"


## begin job

# create job folder
rm -rf $WORKDIR
mkdir $WORKDIR
# copy run-script into job directory
cp $INIDIR/run_$JOBNAME.pbs $WORKDIR/

# start timing
echo
echo ' *** Start Time *** '
date
echo 

# save job script in workdir
cp $INIDIR/exec_pre-processing.sh $WORKDIR
# launch job script
cd $WORKDIR
./exec_pre-processing.sh

# end timing
echo
echo ' *** End Time *** '
date
echo
