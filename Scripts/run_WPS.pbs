#!/bin/bash
#MOAB/Torque submission script for SciNet GPC

## queue/PBS settings
#PBS -l nodes=1:m128g:ppn=16
#PBS -l walltime=0:30:00
# std and error output
# #PBS -j oe
#PBS -o $PBS_JOBNAME.$PBS_JOBID.out
#PBS -e $PBS_JOBNAME.$PBS_JOBID.err
# send email if abort (nbae)
#PBS -M aerler@atmosp.physics.utoronto.ca
#PBS -m ae
# assemble job name
#PBS -N WPS
## submit to queue (NB: this has to be the last PBS line!)
# batch (default), debug, largemem
#PBS -q largemem


## job settings

# optional arguments
export RUNPYWPS=1
export RUNREAL=1
# folders: $METDATA, $REALIN, $RAMIN, $REALOUT, $RAMOUT
export RAMIN=1
export RAMOUT=1
export METDATA="${INIDIR}/metgrid/"
#export REALOUT="${INIDIR}/wrfinput/"

# run configuration
export TASKS=16 # number of MPI task
export THREADS=1 # number of OpenMP threads
# directory setup
export INIDIR=$PBS_O_WORKDIR
export JOBNAME=$PBS_JOBNAME
export WORKDIR=$INIDIR/$JOBNAME/
export RAMDISK="/dev/shm/aerler/"


## setup job environment
hostname
uname

# load modules
module purge
module load intel/12.1.3 intelmpi/4.0.3.008 hdf5/187-v18-serial-intel netcdf/4.1.3_hdf5_serial-intel
module load gcc/4.6.1 centos5-compat/lib64 ncl/6.0.0 python/2.7.2 # pyWPS.py specific modules
module list
# unlimit stack size (unfortunately necessary with WRF to prevent segmentation faults)
ulimit -s unlimited

# set up hybrid envionment: OpenMP and MPI (Intel)
#export KMP_AFFINITY=verbose,granularity=thread,compact
#export I_MPI_PIN_DOMAIN=omp
export I_MPI_DEBUG=1 # less output (currently no problems) 
# launch Intel hybrid (mpi/openmp) job
export TIMING="time -p"
export HYBRIDRUN="mpirun -ppn $TASKS -np $TASKS" # only one node for WPS!


## begin job

# start timing
echo
echo ' *** Start Time *** '
date
echo 

# create job folder
rm -rf $WORKDIR
mkdir $WORKDIR

# run script
./execWPS.sh

# copy driver script into work dir
cp "${INIDIR}/execWPS.sh" "${WORKDIR}"
cp "${INIDIR}/$0" "${WORKDIR}"
 
# end timing
echo
echo ' *** End Time *** '
date
echo
